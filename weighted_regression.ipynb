{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Regression in `python`                                   :jupyter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $T$ and $u$ are &ldquo;independent&rdquo; (or at least\n",
    "orthogonal) variables means that if we want to compute a\n",
    "&ldquo;classical&rdquo; regression we&rsquo;d do it something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define independent random variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "k = 3 # Number of observables in T\n",
    "\n",
    "mu = [0]*k\n",
    "Sigma=[[1,0.5,0],\n",
    "       [0.5,2,0],\n",
    "       [0,0,3]]\n",
    "\n",
    "T = multivariate_normal(mu,Sigma)\n",
    "\n",
    "u = multivariate_normal(cov=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scipy.stats._multivariate.multivariate_normal_frozen at 0x784278617c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define `X`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $X$ can depend on $T$ and $u$.  This dependence needn&rsquo;t be\n",
    "linear!  For example, suppose $X=T^3D + u$, where $D$ is an\n",
    "$\\ell\\times k$ matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a sample of observables $(y,X,T)$ we just use the regression equation,\n",
    "      plus an assumption about the value of $\\beta$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [1/2,1, 2]\n",
    "\n",
    "D = np.random.random(size=(3,3)) # Generate random 3x2 matrix\n",
    "\n",
    "N=1000 # Sample size\n",
    "\n",
    "# Now: Transform rvs into a sample\n",
    "T_sample = T.rvs(N)\n",
    "\n",
    "u = u.rvs(N) # Replace u with a sample\n",
    "\n",
    "X = (T_sample**3)@D  # Note use of ** operator for exponentiation\n",
    "\n",
    "y = X@beta + u # Note use of @ operator for matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn to estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have data on *realizations* $(y,X,T)$.  Now forget\n",
    "     that we know $\\beta$ and let&rsquo;s estimate it, using weighted least\n",
    "     squares.  As a numerical matter it&rsquo;s better to avoid explicitly\n",
    "     inverting the $(T^T X)$ matrix; instead we can solve the &ldquo;normal&rdquo;\n",
    "     equations\n",
    "\n",
    "\\begin{align*}\n",
    "   T'y &= T' X b + T' u\\\\\n",
    "   \\mbox{E}(T'u) = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classical case we were trying to solve a linear system that\n",
    " took the form $Ab=0$, with $A$ a square matrix.  In the present case\n",
    " we&rsquo;re also trying to solve a linear system, but with a matrix $A$\n",
    " that may have more rows than columns.  Provided the rows are linearly\n",
    " independent, this implies that we have an **overidentified** system of\n",
    " equations.  We&rsquo;ll return to the implications of this later, but for\n",
    " now this also calls for a different numerical approach, using\n",
    " `np.linalg.lstsq` instead of `np.linalg.solve`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'multivariate_normal_frozen' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv, sqrtm\n\u001b[0;32m----> 3\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mlstsq(\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129m@X\u001b[39m,T\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@y\u001b[39m,rcond\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# lstsq returns several results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m e \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m X\u001b[38;5;129m@b\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'multivariate_normal_frozen' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import inv, sqrtm\n",
    "\n",
    "b = np.linalg.lstsq(T.T@X,T.T@y,rcond=None)[0] # lstsq returns several results\n",
    "\n",
    "e = y - X@b\n",
    "\n",
    "print(b)\n",
    "\n",
    "TXplus = np.linalg.pinv(T.T@X) # Moore-Penrose pseudo-inverse\n",
    "\n",
    "# Covariance matrix of b\n",
    "vb = e.var()*TXplus@T.T@T@TXplus.T  # u is known to be homoskedastic\n",
    "\n",
    "print(vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical solution to the weighted regression problem involves computing `lstsq(T.T@X,T.T@y)`, i.e., the regression of $y^*=T'y$ on $X^*=T'X$.  How many observations are there in $(X^*,y^*)$?    How would you interpret this regression?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
