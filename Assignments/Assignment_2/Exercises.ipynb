{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5e1791-bc29-4e15-89ee-15a8916cb378",
   "metadata": {},
   "source": [
    "#### 1. Evaluate the truth of following statement: In the linear regression y = Xβ + u the usual identifying assumption E(u|X) = 0 (call this an assumption of mean independence) implies E(h(X) · u) = 0 for any function h satisfying some regularity conditions related to measurability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831664e6-02f5-4008-9d4a-df00ce3e1061",
   "metadata": {},
   "source": [
    "This is true because of the law of iterated expectations.\n",
    "$$ E(h(X) \\cdot u) = E(E(h(X) \\cdot u | X)) $$\n",
    "Since $$ E(u|X) = 0 $$ this becomes\n",
    "$$ E(E(h(X) \\cdot 0 | X)) $$\n",
    "$$ E(E(0 | X)) = 0 $$\n",
    "\n",
    "** do we need to say anything about the regularity conditions here for this to hold true? (h(x) is non-negative? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e51970-5306-4010-a752-cc5aead47327",
   "metadata": {},
   "source": [
    "#### 2. Suppose y, x and u are scalar random variables, with y and x observed but u unobserved. Consider the function h(x) = $x^3$; under standard assumptions this satisfies our concerns about measurability, so E(u|x) = 0 implies E(u$x^3$) = 0. Use this last condition to motivate a simple least squares estimator of the regression equation y = α + βx + u. How does this differ from the usual OLS estimator? Why might one prefer one to the other, and under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3c686-9531-4e60-8040-c0f39ea5178a",
   "metadata": {},
   "source": [
    "First, multiply both sides of the regression equation by $x^3$ to obtain\n",
    "$$ y \\cdot x^3= α\\cdot x^3 + βx\\cdot x^3 + u\\cdot x^3 $$\n",
    "When $E(u|x)=0$, then $E(ux^3)=0$. So,\n",
    "$$ E(y \\cdot x^3)= E(α\\cdot x^3) + E(βx\\cdot x^3) + E(u\\cdot x^3) = α\\cdot E(x^3) + β\\cdot E(x^4) $$\n",
    "This is different from the usual OLS estimator because it doesn't contain an error term and relies on an assumption about the form of u.\n",
    "We might prefer this simple least squares estimator when we know something about u and can credibly claim $E(u\\cdot h(X))=0$. This estimator provides greater consistency (assuming the assumption about u is accurate). We would prefer normal OLS when we don't know anything about u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d467e-f246-4d82-b8ff-668fc6bf8ac0",
   "metadata": {},
   "source": [
    "#### 3. Sometimes we will encounter estimators (e.g., Maximum likelihood) that adopt an assumption of independence, rather than mean independence. In the current setting this might be expressed as something like $Pr(x < x ∩ u < u) = F(x)G(u)$ for some cumulative distribution functions F and G. Show that independence implies mean independence, but not the converse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be1ab3",
   "metadata": {},
   "source": [
    "$$ E[Pr(x < x \\cap u < u)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} Pr(x < x \\cap u < u) \\cdot f(x, u) \\, du \\, dx $$\n",
    "\n",
    "Given x and u are independent, we know: \n",
    "$$ Pr(x < x ∩ u < u) = F(x)G(u) $$\n",
    "\n",
    "Hence, the joint pdf of x and u can be written as product of their individual pdfs. The expression them simplifies to the product of E[x] and E[u]:\n",
    "$$ E[F(x)G(u)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} F(x)f_x(x) dx \\cdot G(u)g_u(u) \\, du = E[x] \\cdot E[u] $$\n",
    "\n",
    "Therefore:\n",
    "$$ E[x|u] = E[x] $$\n",
    "\n",
    "Hence, we show that independence implies mean independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baad3d7",
   "metadata": {},
   "source": [
    "Mean independence does not imply independence. Counterexample: consider a binary random variable X and random variable U. When x = 0, the value of u is always equal to 0. When x = 1, u is distributed normally with mean 0 and some non-zero variance sigma. The conditional expectation of U given X is equal to the unconditional  expectation of U, which is 0 (mean independence is satisfied). However, these random variables are not independent. If we observe a non-zero value u, then we know the associated x must be equal to 1. Independence between u and x implies that the cdf of u given x remains consistent for each x value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
