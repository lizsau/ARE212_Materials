{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5e1791-bc29-4e15-89ee-15a8916cb378",
   "metadata": {},
   "source": [
    "#### 1. Evaluate the truth of following statement: In the linear regression y = Xβ + u the usual identifying assumption E(u|X) = 0 (call this an assumption of mean independence) implies E(h(X) · u) = 0 for any function h satisfying some regularity conditions related to measurability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831664e6-02f5-4008-9d4a-df00ce3e1061",
   "metadata": {},
   "source": [
    "This is true because of the law of iterated expectations.\n",
    "$$ E(h(X) \\cdot u) = E(E(h(X) \\cdot u | X)) $$\n",
    "Since $$ E(u|X) = 0 $$ this becomes\n",
    "$$ E(E(h(X) \\cdot 0 | X)) $$\n",
    "$$ E(E(0 | X)) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e51970-5306-4010-a752-cc5aead47327",
   "metadata": {},
   "source": [
    "#### 2. Suppose y, x and u are scalar random variables, with y and x observed but u unobserved. Consider the function h(x) = $x^3$; under standard assumptions this satisfies our concerns about measurability, so E(u|x) = 0 implies E(u$x^3$) = 0. Use this last condition to motivate a simple least squares estimator of the regression equation y = α + βx + u. How does this differ from the usual OLS estimator? Why might one prefer one to the other, and under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3c686-9531-4e60-8040-c0f39ea5178a",
   "metadata": {},
   "source": [
    "First, multiply both sides of the regression equation by $x^3$ to obtain\n",
    "$$ y \\cdot x^3= α\\cdot x^3 + βx\\cdot x^3 + u\\cdot x^3 $$\n",
    "When $E(u|x)=0$, then $E(ux^3)=0$. So,\n",
    "$$ E(y \\cdot x^3)= E(α\\cdot x^3) + E(βx\\cdot x^3) + E(u\\cdot x^3) = α\\cdot E(x^3) + β\\cdot E(x^4) $$\n",
    "This is different from the usual OLS estimator because it doesn't contain an error term and relies on an assumption about the form of u.\n",
    "We might prefer this simple least squares estimator when we know something about u and can credibly claim $E(u\\cdot h(X))=0$. This estimator provides greater consistency (assuming the assumption about u is accurate). We would prefer normal OLS when we don't know anything about u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d467e-f246-4d82-b8ff-668fc6bf8ac0",
   "metadata": {},
   "source": [
    "#### 3. Sometimes we will encounter estimators (e.g., Maximum likelihood) that adopt an assumption of independence, rather than mean independence. In the current setting this might be expressed as something like $Pr(x < x ∩ u < u) = F(x)G(u)$ for some cumulative distribution functions F and G. Show that independence implies mean independence, but not the converse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba66ab2-2ad6-4eaf-b349-4608eefd9ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
