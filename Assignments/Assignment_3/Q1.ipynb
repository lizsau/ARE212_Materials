{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae47aff-334d-41bf-b1e2-0b8d527bc314",
   "metadata": {},
   "source": [
    "## 1. EXERCISES (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc932d-9ae7-4290-8129-0b3a04c15048",
   "metadata": {},
   "source": [
    "#### When we approach a new estimation problem from a GMM perspective, there's a simple set of steps we can follow.\n",
    "#### - Describe the parameter space $B$;\n",
    "#### - Describe a function $g_j(b)$ such that $\\mathbb{E}g_j(\\beta) = 0$;\n",
    "#### - Describe an estimator for the covariance matrix $\\mathbb{E}g_j(\\beta)g_j(\\beta)^T$.\n",
    "\n",
    "#### 1. Explain how the steps outlined below can be used to construct an optimally weighted GMM estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9af67-0dd0-4af4-bd17-6c253ec104ec",
   "metadata": {},
   "source": [
    "When we approach a new estimation problem from a GMM perspective\n",
    "there’s a simple set of steps we can follow.\n",
    "- Describe the parameter space B;\n",
    "- Describe a function gj(b) such that Egj(β) = 0;\n",
    "- Describe an estimator for the covariance matrix Egj(β)gj(β)<sup>T</sup>.\n",
    "\n",
    "Once we have described the above steps, we do the following:\n",
    "\n",
    "1. Estimate $b_1$ (trial solution)\n",
    "2. Use $b_1$ to estimate $W_2$ where $ W_2= [ \\hat{\\Omega} (b_1) ]^{-1} $\n",
    "3. Calculate $b_2 = \\arg\\min_{b} (N \\cdot g_N(b)\\cdot W_2 \\cdot g_N(b) )$\n",
    "4. Therefore, $\\sqrt{N} \\cdot b_2 \\xrightarrow[]{\\text{d}} N(\\beta, D \\cdot \\Omega^{-1} \\cdot D^T)$\n",
    "5. $J_N (b_2)= (N \\cdot g_N(b_2)\\cdot W_2 \\cdot g_N(b_2))$ is the minimized value function\n",
    "6. $b_2$ is the optimally weighted GMM estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679602b8-c341-45ef-8f74-ecb8e0c7a0a7",
   "metadata": {},
   "source": [
    "#### (3). Consider the following models. For each model, write a data-generating process in **python**. Your function **dgp** should take as arguments a sample size $N$ and a vector of \"true\" parameters $b_0$, and return a dataset $(y,X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b747e1-4ddd-41f8-9897-b6a666e404fa",
   "metadata": {},
   "source": [
    "#### (a). $\\mathbb{E}y = \\mu$; $\\mathbb{E}(y-\\mu)^2 = \\sigma^2; \\mathbb{E}(y-\\mu)^3 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef797b14-ede1-478e-a702-b52826574600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Mean of y: 0.04115040424879694\n",
      "Variance of y: 1.0039873027520383\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dgp_normal(N, mu, sigma):\n",
    "    \"\"\"\n",
    "    Generate a dataset from a normal distribution defined by the model:\n",
    "        y ~ N(mu, sigma^2)\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        mu (float): Mean of the normal distribution.\n",
    "        sigma (float): Standard deviation of the normal distribution.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray): Array of generated samples, each drawn from N(mu, sigma^2).\n",
    "    \"\"\"\n",
    "    # Generate N samples from a normal distribution with mean `mu` and standard deviation `sigma`\n",
    "    y = np.random.normal(loc=mu, scale=sigma, size=N)\n",
    "    \n",
    "    # There are no covariates (X) in this model, just return y\n",
    "    return y\n",
    "\n",
    "# Example usage:\n",
    "N = 1000      # Number of samples to generate\n",
    "mu = 0        # Mean of the normal distribution\n",
    "sigma = 1     # Standard deviation of the normal distribution\n",
    "y = dgp_normal(N, mu, sigma)\n",
    "\n",
    "# Output some statistics to confirm the generation process\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Mean of y:\", np.mean(y))\n",
    "print(\"Variance of y:\", np.var(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcf5e9-3c8f-466a-b5cd-6c14a30720a8",
   "metadata": {},
   "source": [
    "#### (b). $y = \\alpha + X\\beta + u$; with $\\mathbb{E}(X^T u) = \\mathbb{E}(u) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15d963be-5395-46b5-983b-85ecf7abe645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Means of y: 1.012472345967049\n",
      "Covariance between X and residuals (should be near zero):\n",
      "[[ 1.00626695 -0.05914442  0.00120761]\n",
      " [-0.05914442  1.03069175 -0.01375799]\n",
      " [ 0.00120761 -0.01375799  0.26767976]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dgp_linear(N, alpha, beta, sigma, num_predictors):\n",
    "    \"\"\"\n",
    "    Generate a dataset from a linear model:\n",
    "        y = alpha + X*beta + u\n",
    "    where u ~ N(0, sigma^2) and E(X^T*u) = 0 (X is independent of u).\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        alpha (float): Intercept of the linear model.\n",
    "        beta (array-like): Coefficients for the predictors in the linear model.\n",
    "        sigma (float): Standard deviation of the noise.\n",
    "        num_predictors (int): Number of predictors (columns of X).\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray): Response variable.\n",
    "        X (np.ndarray): Predictor variables.\n",
    "    \"\"\"\n",
    "    # Generate predictor data X\n",
    "    X = np.random.normal(size=(N, num_predictors))\n",
    "    \n",
    "    # Generate the noise term u\n",
    "    u = np.random.normal(scale=sigma, size=N)\n",
    "    \n",
    "    # Calculate response variable y\n",
    "    y = alpha + X.dot(beta) + u\n",
    "    \n",
    "    return y, X\n",
    "\n",
    "# Example usage:\n",
    "N = 1000  # Number of samples\n",
    "alpha = 1.0  # Intercept\n",
    "beta = np.array([0.5, -0.25])  # Coefficients for each predictor\n",
    "sigma = 0.5  # Standard deviation of the error term\n",
    "num_predictors = 2  # Number of predictors\n",
    "\n",
    "y, X = dgp_linear(N, alpha, beta, sigma, num_predictors)\n",
    "\n",
    "# Output some statistics to confirm the generation process\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Means of y:\", np.mean(y))\n",
    "print(\"Covariance between X and residuals (should be near zero):\")\n",
    "print(np.cov(X.T, y - (alpha + X.dot(beta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87e65f-aba9-47f7-9117-b68dae0337b6",
   "metadata": {},
   "source": [
    "#### (c). $y = \\alpha + X\\beta + u$; with $\\mathbb{E}(X^T u) = \\mathbb{E}(u) = 0$, and $\\mathbb{E}(u^2) = \\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bf37107-5d36-4e21-a83b-30e1877b96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Mean of y: 0.9740014445812272\n",
      "Variance of residuals (should approximate sigma^2): 0.25093947828623564\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dgp_linear_independent(N, alpha, beta, sigma, num_predictors):\n",
    "    \"\"\"\n",
    "    Generate a dataset from a linear model:\n",
    "        y = alpha + X*beta + u\n",
    "    where u ~ N(0, sigma^2) and E(X^T*u) = 0 (X is independent of u).\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        alpha (float): Intercept of the linear model.\n",
    "        beta (array-like): Coefficients for the predictors in the linear model.\n",
    "        sigma (float): Standard deviation of the noise.\n",
    "        num_predictors (int): Number of predictors (columns of X).\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray): Response variable.\n",
    "        X (np.ndarray): Predictor variables.\n",
    "    \"\"\"\n",
    "    # Generate predictor data X\n",
    "    X = np.random.normal(size=(N, num_predictors))\n",
    "    \n",
    "    # Generate the noise term u\n",
    "    u = np.random.normal(scale=sigma, size=N)\n",
    "    \n",
    "    # Calculate response variable y\n",
    "    y = alpha + X.dot(beta) + u\n",
    "    \n",
    "    return y, X\n",
    "\n",
    "# Example usage:\n",
    "N = 1000  # Number of samples\n",
    "alpha = 1.0  # Intercept\n",
    "beta = np.array([0.5, -0.25])  # Coefficients for each predictor\n",
    "sigma = 0.5  # Standard deviation of the error term\n",
    "num_predictors = 2  # Number of predictors\n",
    "\n",
    "y, X = dgp_linear_independent(N, alpha, beta, sigma, num_predictors)\n",
    "\n",
    "# Output some statistics to confirm the generation process\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Mean of y:\", np.mean(y))\n",
    "print(\"Variance of residuals (should approximate sigma^2):\", np.var(y - (alpha + X.dot(beta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063f31c-5428-4c6b-99e1-f03e22a42fee",
   "metadata": {},
   "source": [
    "#### (d). $y = \\alpha + X\\beta + u$; with $\\mathbb{E}(X^T u) = \\mathbb{E}(u) = 0$, and $\\mathbb{E}(u^2) = e^{X\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef6dc235-8644-47fe-90d9-9672f9d3b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Mean of y: 1.0394721327653837\n",
      "Variances of u by sampled X0: 1.0631687975040318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dgp_heteroscedastic(N, alpha, beta, X0_coefficient, num_predictors):\n",
    "    \"\"\"\n",
    "    Generate a dataset from a linear model with heteroscedastic errors:\n",
    "        y = alpha + X*beta + u\n",
    "    where E(X^T*u) = E(u) = 0, and E(u^2) = exp(X0).\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        alpha (float): Intercept of the linear model.\n",
    "        beta (array-like): Coefficients for the predictors in the linear model.\n",
    "        X0_coefficient (float): Coefficient to scale X0 in the exp function for variance.\n",
    "        num_predictors (int): Number of predictors (columns of X), including X0.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray): Response variable.\n",
    "        X (np.ndarray): Predictor variables.\n",
    "    \"\"\"\n",
    "    # Generate predictor data X\n",
    "    X = np.random.normal(size=(N, num_predictors))\n",
    "    \n",
    "    # Compute the variance for each observation based on X0\n",
    "    variances = np.exp(X[:, 0] * X0_coefficient)\n",
    "    \n",
    "    # Generate the noise term u with heteroscedastic variance\n",
    "    u = np.random.normal(scale=np.sqrt(variances), size=N)\n",
    "    \n",
    "    # Calculate response variable y\n",
    "    y = alpha + X.dot(beta) + u\n",
    "    \n",
    "    return y, X\n",
    "\n",
    "# Example usage:\n",
    "N = 1000                    # Number of samples\n",
    "alpha = 1.0                 # Intercept\n",
    "beta = np.array([0.5, -0.25, 0.3])  # Coefficients for each predictor\n",
    "X0_coefficient = 0.1        # Coefficient for the exponent in variance calculation\n",
    "num_predictors = 3          # Number of predictors\n",
    "\n",
    "y, X = dgp_heteroscedastic(N, alpha, beta, X0_coefficient, num_predictors)\n",
    "\n",
    "# Output some statistics to confirm the generation process\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Mean of y:\", np.mean(y))\n",
    "print(\"Variances of u by sampled X0:\", np.var(y - (alpha + X.dot(beta))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e9815-a166-448a-b6b0-ade12c1b538a",
   "metadata": {},
   "source": [
    "#### (e). $y = \\alpha + X\\beta + u$; with $\\mathbb{E}(Z^T u) = \\mathbb{E}(u) = 0$ and $\\mathbb{E}(Z^T X) = Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60b0fb7a-7245-4eb0-9c56-9f61ad512e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Means of y: 1.0090078961943065\n",
      "Covariance Z'X (sample):\n",
      "[[0.91546763 0.40891644]\n",
      " [0.45723931 1.01211337]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dgp_instrumental(N, alpha, beta, sigma, Q, num_instruments, num_predictors):\n",
    "    \"\"\"\n",
    "    Generate a dataset for an instrumental variables model:\n",
    "        y = alpha + X*beta + u\n",
    "    where E(Z^T*u) = 0 (Z is uncorrelated with u) and E(Z^T*X) = Q.\n",
    "    \n",
    "    Parameters:\n",
    "        N (int): Number of samples to generate.\n",
    "        alpha (float): Intercept of the linear model.\n",
    "        beta (array-like): Coefficients for the predictors in the linear model.\n",
    "        sigma (float): Standard deviation of the noise.\n",
    "        Q (np.ndarray): Expected value of Z^T*X.\n",
    "        num_instruments (int): Number of instrumental variables.\n",
    "        num_predictors (int): Number of predictors in X.\n",
    "        \n",
    "    Returns:\n",
    "        y (np.ndarray): Response variable.\n",
    "        X (np.ndarray): Predictor variables.\n",
    "        Z (np.ndarray): Instrumental variables.\n",
    "    \"\"\"\n",
    "    # Generate instrumental variables Z\n",
    "    Z = np.random.normal(size=(N, num_instruments))\n",
    "    \n",
    "    # Directly calculate X to better control its relationship with Z\n",
    "    # Using a more stable method than pinv if needed or adjusting how X is derived\n",
    "    X = Z @ np.linalg.pinv(Z.T @ Z) @ (Z.T @ Z @ Q[:num_instruments, :num_predictors])\n",
    "\n",
    "    # Ensure X and Z meet the required relationship\n",
    "    if not np.allclose(Z.T @ X / N, Q[:num_instruments, :num_predictors], atol=1e-1):\n",
    "        print(\"Warning: Z'X does not closely match Q. Adjusting...\")\n",
    "    \n",
    "    # Generate the noise term u, independent of Z\n",
    "    u = np.random.normal(scale=sigma, size=N)\n",
    "    \n",
    "    # Calculate response variable y\n",
    "    y = alpha + X.dot(beta) + u\n",
    "    \n",
    "    return y, X, Z\n",
    "\n",
    "# Example usage:\n",
    "N = 1000                      # Number of samples\n",
    "alpha = 1.0                   # Intercept\n",
    "beta = np.array([0.5, -0.25]) # Coefficients for each predictor\n",
    "sigma = 1.0                   # Standard deviation of the error term\n",
    "num_instruments = 2           # Number of instruments\n",
    "num_predictors = 2            # Number of predictors\n",
    "Q = np.array([[1, 0.5],       # Expected value of Z'X\n",
    "              [0.5, 1]])\n",
    "\n",
    "y, X, Z = dgp_instrumental(N, alpha, beta, sigma, Q, num_instruments, num_predictors)\n",
    "\n",
    "# Output some statistics to confirm the generation process\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Means of y:\", np.mean(y))\n",
    "print(\"Covariance Z'X (sample):\")\n",
    "print((Z.T @ X) / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933ed1a-3d5b-4d51-a092-24133f65bed5",
   "metadata": {},
   "source": [
    "#### (f). $y = f(X, \\beta) + u$; with $f$ a known scalar function and with $\\mathbb{E}(Z^T u) = \\mathbb{E}(u) = 0$ and $\\mathbb{E}Z^T X f'(X, \\beta) = Q(\\beta)$. (**Bonus question:** Where does this last restriction come from, and what role does it play?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51104b20-9eec-43af-bbdf-5487deb33353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Means of y: 1.1736100437703674\n",
      "Covariance Z'Xf': [-0.05653119 -0.03701598]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(N, beta, A):\n",
    "    \"\"\"\n",
    "    Generate data satisfying the given model and constraints.\n",
    "    \n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        beta (np.array): Parameter vector.\n",
    "        A (np.array): Matrix defining the relationship for Q(beta).\n",
    "    \n",
    "    Returns:\n",
    "        X (np.array): Predictor variables.\n",
    "        Z (np.array): Instrumental variables.\n",
    "        y (np.array): Response variable.\n",
    "        u (np.array): Noise terms.\n",
    "    \"\"\"\n",
    "    k = len(beta)  # Number of predictors\n",
    "\n",
    "    # Generate Z and X\n",
    "    Z = np.random.normal(size=(N, k))\n",
    "    X = np.random.normal(size=(N, k))\n",
    "\n",
    "    # Compute f and f'\n",
    "    f = np.exp(X @ beta)\n",
    "    df = X * f[:, None]  # Correct broadcasting by reshaping f\n",
    "\n",
    "    # Define Q(beta) - ensuring it matches expected structure\n",
    "    Q_beta = A @ beta\n",
    "\n",
    "    # Adjust X to satisfy E[Z'Xf'] = Q(beta)\n",
    "    scale = np.linalg.lstsq(Z.T @ df, Q_beta, rcond=None)[0]\n",
    "    X *= scale\n",
    "\n",
    "    # Generate u uncorrelated with Z\n",
    "    mean_Z = np.mean(Z, axis=0)\n",
    "    u = np.random.normal(size=N) - np.tile(mean_Z, (N, 1)).mean(axis=1)  # Adjusted for broadcasting\n",
    "\n",
    "    # Calculate y\n",
    "    y = f + u\n",
    "\n",
    "    return X, Z, y, u\n",
    "\n",
    "# Parameters\n",
    "N = 1000\n",
    "beta = np.array([0.5, -0.2])\n",
    "A = np.array([[1, 0], [0, 1]])  # Simplified example\n",
    "\n",
    "# Generate data\n",
    "X, Z, y, u = generate_data(N, beta, A)\n",
    "\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Means of y:\", np.mean(y))\n",
    "print(\"Covariance Z'Xf':\", (Z.T @ (X * np.exp(X @ beta)[:, None])).mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa944f53-0848-4a8a-8b03-3c1c1a1c86c3",
   "metadata": {},
   "source": [
    "The restriction $\\mathbb{E}Z^T X f'(X, \\beta) = Q(\\beta)$ in the context of a nonlinear model where $y=f(X,\\beta) + u$  is crucial for identifying the model parameters $\\beta$ when standard assumptions (like linearity or normality of residuals) do not hold.\n",
    "This condition emerges from the need to ensure that the instruments $Z$ are not only valid (uncorrelated with the error term $u$) but also relevant in explaining the parameter $\\beta$. The function $f$ describes how the dependent variable $y$ is  generated from the predictors $X$ and the parameters $\\beta$, and $f'(X,\\beta)$ is the derviative of $f$ with respect to $\\beta$, reflecting how sensitive $f$ is to changes in $\\beta$.\n",
    "\n",
    "Role of the Restriction:\n",
    "1. Instrument Relevance: This condition ensures that the instruments $Z$ are relevant for the nonlinear aspects of the model encapsulated by $f'(X,\\beta)$. It goes beyond the standard requirement of instrument exogeneity (instruments being uncorrelated with the error term) to also require that they are informative for identifying the nonlinear effects of $\\beta$.\n",
    "2. Model Identification: In nonlinear models, particularly where $f$ is a complex function, standard models like OLS or IV might not suffice for parameter estimation because they do not account for how changes in $\\beta$ affect $y$ through $f$. This condition helps in pinning down $\\beta$ by using $Z$ to instrument not just for $X$ but specifically for the parts of $X$ that are interacted with the derivative, thus capturing the model's dynamics more effectively.\n",
    "3. Efficiency and Consistency: The condition can improve the effciency and consistency of the GMM estimator by appropriately weighting the more informative parts of the data (i.e., where $X$ and $f'(X,\\beta)$ vary significantly with $\\beta$) through the construction of the weighting matrix in GMM taht relies on $Q(\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514d3b7-5cd9-4873-a0b4-a6f4b590cf91",
   "metadata": {},
   "source": [
    "#### (g). $y = f(X, \\beta) + u$; with $f$ a known function and with $\\mathbb{E}(Z^T u) = \\mathbb{E}(u) = 0$ and $\\mathbb{E}Z^T \\frac{\\partial f(X, \\beta)}{\\partial \\beta} = Q(\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0483e011-131b-4b3d-85a7-64cbdbdbb722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Summary:\n",
      "Means of y: 0.6757813922070725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(N, beta, Q_function):\n",
    "    k = len(beta)  # Number of predictors\n",
    "\n",
    "    # Generate Z and X\n",
    "    Z = np.random.normal(size=(N, k))\n",
    "    X = np.random.normal(size=(N, k))\n",
    "\n",
    "    # Compute the function f and its derivative with respect to beta\n",
    "    exp_Xb = np.exp(X @ beta)\n",
    "    f = np.log(1 + exp_Xb)\n",
    "    df_dbeta = X * exp_Xb[:, None] / (1 + exp_Xb[:, None])  # Reshape exp_Xb for broadcasting\n",
    "\n",
    "    # Define the expected derivative matrix Q(beta)\n",
    "    expected_derivative = Q_function(beta)\n",
    "\n",
    "    # Adjust Z to meet the condition: E[Z^T * df/dbeta] = Q(beta)\n",
    "    adjustment = np.linalg.lstsq(Z.T @ df_dbeta, expected_derivative, rcond=None)[0]\n",
    "    Z *= adjustment\n",
    "\n",
    "    # Generate the error term u, uncorrelated with Z\n",
    "    mean_Z = np.mean(Z, axis=0)\n",
    "    u = np.random.normal(size=N) - np.tile(mean_Z, (N, 1)).mean(axis=1)  # Adjusted for broadcasting\n",
    "\n",
    "    # Calculate y\n",
    "    y = f + u\n",
    "\n",
    "    return X, Z, y\n",
    "\n",
    "def Q_function(beta):\n",
    "    return beta  # Simplified example\n",
    "\n",
    "# Usage\n",
    "N = 1000\n",
    "beta = np.array([0.5, -0.2])\n",
    "X, Z, y = generate_data(N, beta, Q_function)\n",
    "\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Means of y:\", np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171f9f9-885d-4230-b701-0a45f8598dc2",
   "metadata": {},
   "source": [
    "#### (h). $y^\\gamma = \\alpha + u$, with $y > 0$ and $\\gamma$ a scalar, and $\\mathbb{E}(Z^Tu) = \\mathbb{E}(u) = 0$ and $\\mathbb{E}Z^T \\begin{bmatrix} \\gamma y^{\\gamma-1} \\\\ -1 \\end{bmatrix} = Q(\\gamma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241610f-1327-4d59-8f6f-92d27dba73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(N, alpha, gamma, Q_function):\n",
    "    \"\"\"\n",
    "    Generate data for the model y^gamma = alpha + u, ensuring all operations are valid.\n",
    "    \"\"\"\n",
    "    # Generate error term u, making sure alpha + u is always positive\n",
    "    u = np.random.normal(0, 1, N)  # Standard deviation might need adjustment\n",
    "    while any(alpha + u <= 0):\n",
    "        u = np.random.normal(0, 1, N)  # Redraw any u that results in non-positive alpha + u\n",
    "\n",
    "    # Compute y\n",
    "    y = (alpha + u)**(1/gamma)\n",
    "\n",
    "    # Generate instrumental variables Z\n",
    "    Z = np.random.normal(size=(N, 2))  # Let's assume two instruments for simplicity\n",
    "\n",
    "    # Adjustment to meet the covariance condition\n",
    "    target = Q_function(gamma)\n",
    "    if target.ndim == 1:\n",
    "        target = target[:, np.newaxis]  # Reshape if necessary\n",
    "    adjustment = np.linalg.lstsq(Z, target, rcond=None)[0]\n",
    "    Z *= adjustment\n",
    "\n",
    "    return y, Z, u\n",
    "\n",
    "def Q_function(gamma):\n",
    "    # Return a column vector for compatibility\n",
    "    return np.array([gamma, -1])[:, np.newaxis]\n",
    "\n",
    "# Parameters\n",
    "N = 1000\n",
    "alpha = 1.0  # Adjust if necessary to ensure alpha + u is always positive\n",
    "gamma = 2.0\n",
    "\n",
    "# Generate data\n",
    "y, Z, u = generate_data(N, alpha, gamma, Q_function)\n",
    "\n",
    "print(\"Generated Data Summary:\")\n",
    "print(\"Means of y:\", np.mean(y))\n",
    "print(\"Check conditions, e.g., Covariance:\", np.cov(Z.T, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f8dd7-b11e-4978-a95f-351ab47f15e9",
   "metadata": {},
   "source": [
    "#### 4. Select the most interesting of the data generating processes you developed, and using the code in gmm.py or GMM_class.py (see https://github.com/ligonteaching/ARE212_Materials/) use data from your dgp to analyze the finite sample performance of the corresponding GMM estimator you’ve constructed. Of particular interest is the distribution of your estimator using a sample size N and how this distribution compares with the limiting distribution as $N \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc438b9-5174-4a6d-a08f-825c7e2b3527",
   "metadata": {},
   "source": [
    "#### **Step1: Data Generation**\n",
    "#### I have chosen the model (c): a simple linear model.\n",
    "#### $y = \\alpha + X\\beta + u$\n",
    "#### where $u \\sim N(0,\\sigma^2)$ and $\\mathbb{E}(X^Tu)=0$ ensuring that $X$ is independent of $u$ and $\\mathbb{E}(u^2) = \\sigma^2$.\n",
    "\n",
    "#### **Step2: GMM Estimation**\n",
    "#### For GMM, the moment condition based on my chosen dgp is:\n",
    "#### $\\mathbb{E}(X^T(y-\\alpha-X\\beta)) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae826f6-5aad-4afd-86dc-0677df7cfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def gmm_estimator(X, y, initial_guess):\n",
    "    \"\"\"\n",
    "    A simple GMM estimator that uses the moment conditions of the model:\n",
    "    E[X' * (y - Xb - a)] = 0\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(params):\n",
    "        alpha, beta = params[0], params[1:]\n",
    "        residuals = y - (X @ beta + alpha)\n",
    "        moments = np.mean(X * residuals[:, np.newaxis], axis=0)\n",
    "        return moments @ moments.T  # Simplifying assumption: identity weight matrix\n",
    "\n",
    "    result = opt.minimize(objective, initial_guess, method='BFGS')\n",
    "    return result.x\n",
    "\n",
    "def dgp_linear_independent(N, alpha, beta, sigma, num_predictors):\n",
    "    \"\"\"\n",
    "    Generate data as specified.\n",
    "    \"\"\"\n",
    "    X = np.random.normal(size=(N, num_predictors))\n",
    "    u = np.random.normal(scale=sigma, size=N)\n",
    "    y = alpha + X.dot(beta) + u\n",
    "    return y, X\n",
    "\n",
    "# Parameters for simulation\n",
    "N = 1000\n",
    "alpha_true = 1.0\n",
    "beta_true = np.array([0.5, -0.25])\n",
    "sigma = 1.0\n",
    "num_predictors = 2\n",
    "\n",
    "# Generate data\n",
    "y, X = dgp_linear_independent(N, alpha_true, beta_true, sigma, num_predictors)\n",
    "\n",
    "# GMM estimation\n",
    "initial_guess = np.array([0.0] + [0.0] * num_predictors)\n",
    "estimated_params = gmm_estimator(X, y, initial_guess)\n",
    "print(\"Estimated Parameters:\", estimated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757e143-d10d-4d39-a9da-0e225a3ead2c",
   "metadata": {},
   "source": [
    "#### **Step3: Analyzing Peformance**\n",
    "#### - Generate multiple samples of data at different sizes $N$.\n",
    "#### - Apply the GMM estimator to each sample.\n",
    "#### - Record the estimates and compare how they converge to the true parameters as $N$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbd56b-9210-4fe0-bc3c-e4eb9e168eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 5000, 10000]\n",
    "results = {}\n",
    "\n",
    "for N in sample_sizes:\n",
    "    estimates = []\n",
    "    for _ in range(100):  # Perform 100 simulations at each sample size\n",
    "        y, X = dgp_linear_independent(N, alpha_true, beta_true, sigma, num_predictors)\n",
    "        estimated_params = gmm_estimator(X, y, initial_guess)\n",
    "        estimates.append(estimated_params)\n",
    "    estimates = np.array(estimates)\n",
    "    results[N] = {\n",
    "        'mean': np.mean(estimates, axis=0),\n",
    "        'std': np.std(estimates, axis=0)\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for size, stats in results.items():\n",
    "    print(f\"Sample Size: {size}, Mean of Estimates: {stats['mean']}, Std Deviation: {stats['std']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
